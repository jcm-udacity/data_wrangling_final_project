{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John McGonagle  \n",
    "Udacity Data Analyst Nanodegree  \n",
    "Data Wrangling Final Project  \n",
    "  \n",
    "## 1. Problems During Data Auditing and Cleaning\n",
    "I chose to study an OpenStreetMap dataset spanning Boston, MA and Cambridge, MA (the two cities are across a river from one another).\n",
    "  \n",
    "While I imported all of the data in the map into a MongoDB database, I specifically focused on auditing and cleaning two fields, the street and housenumber fields.   These were originally tagged with the keys 'addr:street' and 'addr:housenumber' respectively, the 'addr' standing for 'address'.  Auditing each revealed some interesting problems that took a mix of programmatic and manual corrections.  Finally, the conversion to JSON revealed a conflict concerning the type field for relations using the original code.\n",
    "\n",
    "### 1A. House Numbers\n",
    "I started my investigation by creating a regular expression to test for standard house numbers between 1 and 5 digits:\n",
    "```python\n",
    "r'^[1-9]\\d{0,4}?$'\n",
    "```\n",
    "\n",
    "This revealed the presence of many non-canonical formats for house numbers, some containing letters or fractions at the end (for multi-unit addresses), commas + semicolons (for locations with mulitple, non-consecutive addresses), hyphens + colons (for locations with multiple, consecutive addresses spanning a range), and non-numeric (e.g. 'Two' instead of '2').  Some examples from my dataset were:\n",
    "\n",
    "```\n",
    "{'11,11A',\n",
    " '111,111A',\n",
    " '114-124',\n",
    " '126-130',\n",
    " '14,16',\n",
    " '14-16',\n",
    " '21;23',\n",
    " '27,27 1/2',\n",
    " '308A',\n",
    " '33,35',\n",
    " '347;349',\n",
    " '35-39',\n",
    " '37R',\n",
    " '38,40',\n",
    " '92,94',\n",
    " 'One'}\n",
    "```\n",
    "\n",
    "I decided that the canonical punctuation would be commas for non-consecutive lists and hyphens for consecutive lists.  This was easiest, as the vast majority of house numbers not satisfying my original regular expression used only these two forms of punctuation.  Furthermore, I decided letter suffixes would have to be capitalized and fractions would have to take the form of '1/2' (which all house numbers did anyway).  Also, no white space would be allowed in the cleaned house number, except in the case of a space separating the substring '1/2'.  Lastly, for non-consecutive lists as many as four house numbers would be permitted, which all existing house numbers satisfied.  The new regular expression was:\n",
    "\n",
    "```python\n",
    "r'^([1-9]\\d{0,4}|0)([A-Z]| 1/2)?((-([1-9]\\d{0,4}|0)([A-Z]| 1/2)?)?|(,([1-9]\\d{0,4}|0)([A-Z]| 1/2)?){0,3})$'\n",
    "```\n",
    "\n",
    "A sample of house numbers that did not satisfy this regular expression were:\n",
    "\n",
    "```\n",
    "{'112:114',\n",
    " '13B;13C',\n",
    " '16;18',\n",
    " '17/19',\n",
    " '250;252;254;256',\n",
    " '25;27;29;29A',\n",
    " '299;301',\n",
    " '2;4',\n",
    " '347;349',\n",
    " '3;5',\n",
    " '4 Suite S-1155',\n",
    " '5;7',\n",
    " '62:64',\n",
    " '8B;10;12;14',\n",
    " '9;11;13;15',\n",
    " 'Building 22',\n",
    " 'PO Box 846028',\n",
    " 'Ten',\n",
    " 'Zero'}\n",
    "```\n",
    "\n",
    "In order to remove those house numbers that could be fixed by replacing semicolons with commas and colons with hyphens, I tried replacing each non-canonical string with the aforementioned replacements and then testing those strings against the above regular expression once again.  After I did this, only the following house numbers remained:\n",
    "\n",
    "```\n",
    "{'17/19',\n",
    " '33, 33 1/2',\n",
    " '34,36a,36b',\n",
    " '4 Suite S-1155',\n",
    " '6; 8',\n",
    " '84; 86',\n",
    " 'One',\n",
    " 'Ten',\n",
    " 'Zero'}\n",
    " \n",
    "{'Building 22',\n",
    " 'PO Box 846028'}\n",
    "```\n",
    "\n",
    "The first set was small enough for me to handle through manual correction (in the form of a dictionary mapping the wrong value to the corrected value), while I considered the second set to be in error, since for those elements the housenumber field did not include actual house number information (the first is an internal college campus designation, while the second is not a street address).  House numbers that were in error were thrown out of the dataset (i.e. not included in the final JSON file).  \n",
    "  \n",
    "The final data cleaning consisted of the following logic:\n",
    "1. If the string is a well-formed house number, return the original string.\n",
    "2. Otherwise, try replacing all the semilcolons with commas and colons with hyphens.  If the string is now canonical, return this new string.\n",
    "3. If the string is in the error set, exit and do not add the element to the final JSON file.\n",
    "4. Otherwise, replace the string with the corresponding value from the manual corrections map.\n",
    "\n",
    "### 1B. Street Names\n",
    "Auditing and cleaning the street field took largely the same form.  I used the original regular expression to extract the last word (making sure to add whitespace at the end of the regular expression, since some street fields in my dataset had whitespace at the end):\n",
    "\n",
    "```python\n",
    "r'\\b\\S+\\.?\\s*$'\n",
    "```\n",
    "\n",
    "Using the values extracted by this regular expression, I compared each to a set of common canonical street types (e.g. 'Road', 'Street', and 'Avenue').  After inspecting the values, I added any common street types not in the canonical list (e.g. 'Boulevard' and 'Court') and reinspected the values.  There were also some locale-specific values that I added to a whitelist for ignoring, the most prominent example being `'Faneuil Hall'`, a popular Boston location.  Some examples of street types not satisfying this stage were:\n",
    "\n",
    "```\n",
    "{'303': {'First Street, Suite 303'},\n",
    " '501': {'Bromfield Street #501'},\n",
    " 'Ave': {'Western Ave', 'Lexington Ave', 'Somerville Ave'},\n",
    " 'Ave.': {'Somerville Ave.'},\n",
    " 'Center': {'Cambridge Center'},\n",
    " 'Ct': {'Kelley Ct'},\n",
    " 'Floor': {'Boylston Street, 5th Floor'},\n",
    " 'Pkwy': {'Birmingham Pkwy'},\n",
    " 'Rd': {'Abby Rd', 'Soldiers Field Rd'}}\n",
    "```\n",
    "\n",
    "Some of the preceding values were just abbreviations (with or without punctuation) of street types in the canonical street type list.  Thus, to correct these, I built a dictionary mapping abbreviations to their canonical form.  Using a regular expression, the canonical type could be substituted for its corresponding abbreviation in the dictionary.  The eventual result of this process was the map:\n",
    "\n",
    "```\n",
    "{'Ave': 'Avenue', \n",
    " 'Ave.': 'Avenue', \n",
    " 'Ct': 'Court',\n",
    " 'Hwy': 'Highway', \n",
    " 'Pkwy': 'Parkway', \n",
    " 'Pl': 'Place', \n",
    " 'rd.': 'Road', \n",
    " 'Rd': 'Road',\n",
    " 'Rd.': 'Road', \n",
    " 'Sq.': 'Square', \n",
    " 'st': 'Street', \n",
    " 'St': 'Street',\n",
    " 'St.': 'Street',\n",
    " 'ST': 'Street'}\n",
    "```\n",
    "\n",
    "Once this was done, I extracted the list of non-satisfying street types into a list:\n",
    "\n",
    "```\n",
    "{'1100': {'First Street, Suite 1100'},\n",
    " '12': {'Harvard St #12'},\n",
    " '1302': {'Cambridge Street #1302'},\n",
    " '1702': {'Franklin Street, Suite 1702'},\n",
    " '3': {'Kendall Square - 3'},\n",
    " '303': {'First Street, Suite 303'},\n",
    " '501': {'Bromfield Street #501'},\n",
    " '6': {'South Station, near Track 6'},\n",
    " '846028': {'PO Box 846028'},\n",
    " 'Albany': {'Albany'},\n",
    " 'Boylston': {'Boylston'},\n",
    " 'Building': {'South Market Building'},\n",
    " 'Cambrdige': {'Cambrdige'},\n",
    " 'Corner': {'Webster Street, Coolidge Corner'},\n",
    " 'Dartmouth': {'Dartmouth'},\n",
    " 'Floor': {'Boylston Street, 5th Floor'},\n",
    " 'Garage': {'Stillings Street Garage'},\n",
    " 'Hampshire': {'Hampshire'},\n",
    " 'LEVEL': {'LOMASNEY WAY, ROOF LEVEL'},\n",
    " 'Lafayette': {'Avenue De Lafayette'},\n",
    " 'Longwood': {'Longwood'},\n",
    " 'Market': {'Faneuil Hall Market'},\n",
    " 'Newbury': {'Newbury'},\n",
    " 'Pasteur': {'Avenue Louis Pasteur'},\n",
    " 'South': {'Charles Street South'},\n",
    " 'Windsor': {'Windsor'},\n",
    " 'Winsor': {'Winsor'},\n",
    " 'floor': {'Sidney Street, 2nd floor', 'First Street, 18th floor'}}\n",
    "```\n",
    "\n",
    "Values in this list fell into two categories, values that could be corrected manually and values that were errors.  Errors were, like the house number errors, values that did not contain street information, such as `'South Station, near Track 6'` and `'PO Box 846028'`.  These were not included in the final JSON file.  Values that could be corrected manually lagely consisted of streets missing street types, such as `'Windsor'` and `'Longwood`'.  Other values were French street names (e.g. `'Avenue De Lafayette`') that had the street type before the street name, values including unit and floor numbers (e.g. `'Cambridge Street #1302'` and `'First Street, 18th floor'`), and values referencing local landmarks (e.g. `'Webster Street, Coolidge Corner'`).  Manual corrections were handled by a dictionary mapping the wrong value to the corrected value.  \n",
    "  \n",
    "A final pass over the data revealed that some street names, while having the correct (in some cases, replaced) street types, also included a series of numbers at the beginning, corresponding to the housenumber.  This information was removed by using a regular expression to substitute the offending number (and optional adjacent whitespace) with the empty string.\n",
    "\n",
    "The final data cleaning consisted of the following logic:\n",
    "1. If the string is a well-formed street, do nothing.\n",
    "2. Otherwise, test for the occurence of a street type abbreviation using the abbreviation-to-canonical dictionary.  If so, replace the abbreviation with the canonical street type.\n",
    "3. If the string is in the error set, exit and do not add the element to the final JSON file.\n",
    "4. Otherwise, replace the string with the corresponding value from the manual corrections map.\n",
    "5. Finally, if the string contains a number (plus optional whitespace) at the beginning of the string, replace them with an empty string `''`.\n",
    "\n",
    "### 1C. JSON Conversion\n",
    "For completeness, I decided to add the 'relation' elements to my MongoDB database.  During the data cleaning and JSON conversion process, I discovered that the original code\n",
    "\n",
    "```python\n",
    "if element.tag == 'node' or element.tag == 'way' or element.tag == 'relation':\n",
    "        node['type'] = element.tag\n",
    "        node['created'] = {}\n",
    "        node['pos'] = [None, None]\n",
    "        attrib = element.attrib\n",
    "```\n",
    "\n",
    "yielded an unexpected value, since some relation elements contain tags with `k=\"type\"`, causing the actual type (i.e. 'relation', the value of `element.tag`) to be overwritten.  For example:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"type\": \"multipolygon\",\n",
    "  \"created\": {\n",
    "    \"changeset\": \"48277481\",\n",
    "    \"timestamp\": \"2017-04-30T09:54:31Z\",\n",
    "    \"uid\": \"665748\",\n",
    "    \"user\": \"sebastic\",\n",
    "    \"version\": \"2\"\n",
    "  }\n",
    "```\n",
    "\n",
    "does not have type equal to `'node'`, `'way'`, or `'relation'`, as it would appear to require, given the logic above.  The error is due to the blanket assignment statement `node[key] = value` later in the inner element for loop, specficially when the `key` variable equals `type`.  \n",
    "  \n",
    "Changing the above code to:\n",
    "\n",
    "```python\n",
    "if element.tag == 'node' or element.tag == 'way' or element.tag == 'relation':\n",
    "        node['tag'] = element.tag\n",
    "        node['created'] = {}\n",
    "        node['pos'] = [None, None]\n",
    "        attrib = element.attrib\n",
    "```\n",
    "\n",
    "fixed the problem.\n",
    "\n",
    "I also added the field `'addressstring'` to handle the rare case that a node has both an address tag (usually a string of the full adress, such as `'123 Smith Street, Boston, MA 02215'`) and tags of the form 'addr:[some address subfield]'.  This preserves as much address information as possible in the case that the address subfields are incomplete compared to the address field.\n",
    "  \n",
    "## 2. Data Overview\n",
    "After importing the JSON file into a MongoDB database, I ran multiple queries to calculate several different statistics of the dataset.\n",
    "### 2A. Dataset Statistics\n",
    "#### File Sizes\n",
    "| Filename                  | Size   |\n",
    "| ------------------------- |-------:|\n",
    "| boston_cambridge.osm      | 69MB   |\n",
    "| boston_cambridge_osm.json | 93MB   |\n",
    "  \n",
    "#### Number Of Top Level Elements\n",
    "```python\n",
    "osm_data.find().count()\n",
    "```\n",
    "```\n",
    "Out: 342355\n",
    "```\n",
    "#### Number Of Nodes\n",
    "```python\n",
    "osm_data.find({'tag': 'node'}).count()\n",
    "```\n",
    "```\n",
    "Out: 294037\n",
    "```\n",
    "#### Number Of Ways\n",
    "```python\n",
    "osm_data.find({'tag': 'way'}).count()\n",
    "```\n",
    "```\n",
    "Out: 47703\n",
    "```\n",
    "#### Number Of Relations\n",
    "```python\n",
    "osm_data.find({'tag': 'relation'}).count()\n",
    "```\n",
    "```\n",
    "Out: 615\n",
    "```\n",
    "#### Number Of Distinct Users\n",
    "```python\n",
    "len(osm_data.find().distinct('created.user'))\n",
    "```\n",
    "```\n",
    "Out: 885\n",
    "```\n",
    "### 2B. Other Interesting Statistics\n",
    "#### Top 20 Most Popular Types Of Nodes \n",
    "```python\n",
    "pipeline = [{'$match': {'tag': 'node', 'type': {'$exists': True}}}, \n",
    "            {'$group': {'_id': '$type', 'count': {'$sum': 1}}}, \n",
    "            {'$sort': {'count': -1, '_id': 1}}, \n",
    "            {'$limit': 20}]\n",
    "\n",
    "pprint.pprint(list(osm_data.aggregate(pipeline)))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': 'Special', 'count': 27},\n",
    "      {'_id': 'Academic', 'count': 19},\n",
    "      {'_id': 'Public', 'count': 16},\n",
    "      {'_id': 'Private', 'count': 14},\n",
    "      {'_id': 'School', 'count': 11},\n",
    "      {'_id': 'Charter', 'count': 3},\n",
    "      {'_id': 'Special-Law', 'count': 3},\n",
    "      {'_id': 'Approved Special Education', 'count': 2},\n",
    "      {'_id': 'County', 'count': 2},\n",
    "      {'_id': 'Special-Institutional', 'count': 2},\n",
    "      {'_id': 'Special-Medical', 'count': 2},\n",
    "      {'_id': 'Collaborative Program', 'count': 1},\n",
    "      {'_id': 'private', 'count': 1},\n",
    "      {'_id': 'video', 'count': 1}]\n",
    "```\n",
    "#### Top 20 Most Popular Amenities\n",
    "```python\n",
    "pipeline = [{'$match': {'tag': 'node', 'amenity': {'$exists': True}}}, \n",
    "            {'$group': {'_id': '$amenity', 'count': {'$sum': 1}}}, \n",
    "            {'$sort': {'count': -1, '_id': 1}}, \n",
    "            {'$limit': 20}]\n",
    "\n",
    "pprint.pprint(list(osm_data.aggregate(pipeline)))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': 'restaurant', 'count': 470},\n",
    "      {'_id': 'bench', 'count': 415},\n",
    "      {'_id': 'bicycle_parking', 'count': 202},\n",
    "      {'_id': 'cafe', 'count': 184},\n",
    "      {'_id': 'library', 'count': 140},\n",
    "      {'_id': 'fast_food', 'count': 114},\n",
    "      {'_id': 'school', 'count': 107},\n",
    "      {'_id': 'bicycle_rental', 'count': 97},\n",
    "      {'_id': 'place_of_worship', 'count': 92},\n",
    "      {'_id': 'fountain', 'count': 63},\n",
    "      {'_id': 'post_box', 'count': 62},\n",
    "      {'_id': 'waste_basket', 'count': 62},\n",
    "      {'_id': 'bank', 'count': 59},\n",
    "      {'_id': 'bar', 'count': 51},\n",
    "      {'_id': 'pub', 'count': 46},\n",
    "      {'_id': 'atm', 'count': 36},\n",
    "      {'_id': 'pharmacy', 'count': 35},\n",
    "      {'_id': 'parking', 'count': 28},\n",
    "      {'_id': 'drinking_water', 'count': 27},\n",
    "      {'_id': 'bicycle_repair_station', 'count': 24}]\n",
    "```\n",
    "#### Banks With The Most ATMs\n",
    "```python\n",
    "pipeline = [{'$match': {'tag': 'node', 'amenity': 'bank', 'atm': 'yes'}}, \n",
    "            {'$group': {'_id': '$name', 'count': {'$sum': 1}}}, \n",
    "            {'$sort': {'count': -1, '_id': 1}}, \n",
    "            {'$limit': 20}]\n",
    "\n",
    "pprint.pprint(list(osm_data.aggregate(pipeline)))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': 'Bank of America', 'count': 6},\n",
    "      {'_id': 'Citizens Bank', 'count': 5},\n",
    "      {'_id': 'Santander', 'count': 3},\n",
    "      {'_id': 'Cambridge Trust Company', 'count': 2},\n",
    "      {'_id': 'Citibank', 'count': 2},\n",
    "      {'_id': 'Eastern Bank', 'count': 2},\n",
    "      {'_id': 'Admirals Bank', 'count': 1},\n",
    "      {'_id': 'Cambridge Savings Bank', 'count': 1},\n",
    "      {'_id': 'Cambridge Trust', 'count': 1},\n",
    "      {'_id': \"Citizen's Bank\", 'count': 1},\n",
    "      {'_id': 'MIT Federal Credit Union', 'count': 1},\n",
    "      {'_id': \"People's United Bank\", 'count': 1},\n",
    "      {'_id': 'TD Bank', 'count': 1}]\n",
    "```\n",
    "#### Top 20 Most Popular Cuisines\n",
    "```python\n",
    "pipeline = [{'$match': {'tag': 'node', 'cuisine': {'$exists': True}}}, \n",
    "            {'$group': {'_id': '$cuisine', 'count': {'$sum': 1}}}, \n",
    "            {'$sort': {'count': -1, '_id': 1}}, \n",
    "            {'$limit': 20}]\n",
    "\n",
    "pprint.pprint(list(osm_data.aggregate(pipeline)))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': 'coffee_shop', 'count': 49},\n",
    "      {'_id': 'pizza', 'count': 42},\n",
    "      {'_id': 'mexican', 'count': 34},\n",
    "      {'_id': 'sandwich', 'count': 32},\n",
    "      {'_id': 'american', 'count': 29},\n",
    "      {'_id': 'italian', 'count': 22},\n",
    "      {'_id': 'chinese', 'count': 19},\n",
    "      {'_id': 'burger', 'count': 17},\n",
    "      {'_id': 'asian', 'count': 15},\n",
    "      {'_id': 'donut', 'count': 15},\n",
    "      {'_id': 'thai', 'count': 15},\n",
    "      {'_id': 'indian', 'count': 14},\n",
    "      {'_id': 'ice_cream', 'count': 13},\n",
    "      {'_id': 'japanese', 'count': 13},\n",
    "      {'_id': 'international', 'count': 6},\n",
    "      {'_id': 'regional', 'count': 6},\n",
    "      {'_id': 'seafood', 'count': 6},\n",
    "      {'_id': 'sushi', 'count': 6},\n",
    "      {'_id': 'mediterranean', 'count': 5},\n",
    "      {'_id': 'french', 'count': 4}]\n",
    "```\n",
    "#### Top 20 Users With The Most Created Elements\n",
    "```python\n",
    "pipeline = [{'$group': {'_id': '$created.user', 'count': {'$sum': 1}}}, \n",
    "            {'$sort': {'count': -1, '_id': 1}}, \n",
    "            {'$limit': 20}]\n",
    "\n",
    "pprint.pprint(list(osm_data.aggregate(pipeline)))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': 'crschmidt', 'count': 144099},\n",
    "      {'_id': 'ryebread', 'count': 34851},\n",
    "      {'_id': 'wambag', 'count': 32600},\n",
    "      {'_id': 'jremillard-massgis', 'count': 30442},\n",
    "      {'_id': 'mapper999', 'count': 12981},\n",
    "      {'_id': 'morganwahl', 'count': 12871},\n",
    "      {'_id': 'OceanVortex', 'count': 7705},\n",
    "      {'_id': 'MassGIS Import', 'count': 4059},\n",
    "      {'_id': 'JasonWoof', 'count': 3907},\n",
    "      {'_id': 'JessAk71', 'count': 3655},\n",
    "      {'_id': 'Utible', 'count': 2857},\n",
    "      {'_id': 'Shannon Kelly', 'count': 1935},\n",
    "      {'_id': 'Alexey Lukin', 'count': 1893},\n",
    "      {'_id': 'Ahlzen', 'count': 1814},\n",
    "      {'_id': 'cspanring', 'count': 1796},\n",
    "      {'_id': 'fiveisalive', 'count': 1661},\n",
    "      {'_id': 'probiscus', 'count': 1414},\n",
    "      {'_id': 'phyzome', 'count': 1325},\n",
    "      {'_id': 'synack', 'count': 1322},\n",
    "      {'_id': 'Extant', 'count': 1164}]\n",
    "```\n",
    "#### Histogram Of Number Of Edits\n",
    "```python\n",
    "pipeline = [{'$group': {'_id': '$created.version', 'count': {'$sum': 1}}}, \n",
    "            {'$sort': {'count': -1, '_id': 1}}, \n",
    "            {'$limit': 50}]\n",
    "\n",
    "version_dist = list(osm_data.aggregate(pipeline))\n",
    "hist = [(vd['_id'], vd['count']) for vd in version_dist]\n",
    "```\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# take log log plot of histogram, roughly straight line implies power law\n",
    "plt.plot(*zip(*hist), 'o')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Power Law Distribution Of \\nNumber Of Edits By Count')\n",
    "plt.xlabel('Number of Edits')\n",
    "plt.ylabel('Count');\n",
    "```\n",
    "![Log Log Plot of Edits by Count](../images/version_histogram.png)\n",
    "#### Most Edited Elements\n",
    "```python\n",
    "max_version = osm_data.find_one(sort=[('created.version', -1)])['created']['version']\n",
    "\n",
    "pprint.pprint(list(osm_data.find({'created.version': max_version}, ['name', 'tag', 'created.version'])))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': ObjectId('59bceb1c9d250b2eb76c0164'),\n",
    "       'created': {'version': 463},\n",
    "       'name': 'United States of America',\n",
    "       'tag': 'relation'}]\n",
    "```\n",
    "Since the map of Boston/Cambridge contains the United States border (Boston/Cambridge is on the ocean), the local city map contains the United States boundary relation.  Since this relation defines the outer edge of the entire United States, it is most likely being edited by many members working on local sections of areas on the US border.  This explains the high number of edits.  A similar phenomenon should be seen in state and city boundaries as well.  It would not be too far fetched to propose that the number of edits for a boundary relation would be proportional to the boundary's length.\n",
    "  \n",
    "## 3. Potential Improvements\n",
    "\n",
    "### 3A. Data Gathering Process\n",
    "\n",
    "Since most smartphones these days are GPS-enabled and machine vision has reached a point where scalable image recognition is possible, I imagine an automated crowdsourcing technique could be used to increase the amount and reliability of location tags.  While it is not possible, using only the given dataset, to verify the accuracy of tag information, it is easy to see that existing tags are inconsistent between locations of a similar type (e.g. restaurants or cafes). For example, the following query\n",
    "\n",
    "```\n",
    "from bson.code import Code\n",
    "\n",
    "# define javascript map function\n",
    "# basically an unwind on each document's dictionary keys/fields\n",
    "map_function = Code(\"\"\"\n",
    "                    function() {\n",
    "                        for (var tag_name in this) {\n",
    "                            emit(tag_name, 1);\n",
    "                        }\n",
    "                    }\n",
    "                    \"\"\")\n",
    "\n",
    "# define javascript reduce function\n",
    "# count the number of each tag\n",
    "reduce_function = Code(\"\"\"\n",
    "                       function(tag_name, tag_count) {\n",
    "                           return Array.sum(tag_count);\n",
    "                       }\n",
    "                       \"\"\")\n",
    "\n",
    "# perform the map reduce and name the resulting collectoin 'cafe_tags_pct'\n",
    "# use the query argument to filter out all documents that aren't cafes before performing the map reduce\n",
    "osm_data.map_reduce(map_function, \n",
    "                    reduce_function, \n",
    "                    out='cafe_tags_pct',\n",
    "                    query={'amenity': 'cafe'})\n",
    "                          \n",
    "ctp = db.cafe_tags_pct\n",
    "\n",
    "# used for calculating the percentage\n",
    "num_cafes = ctp.find_one({'_id': '_id'})['value']\n",
    "\n",
    "# 1. Take out known common tags ('_id' and 'amenity', by the above map reduce query operation)\n",
    "# 2. Calculate the percentage as the count of tags divided by the total number of cafe documents\n",
    "# 3. Sort by percentage\n",
    "# 4. Format to remove excessive precision and add '%' symbol\n",
    "pipeline = [{'$match': {'_id': {'$nin': ['_id', 'amenity']}}}, \n",
    "            {'$project': {'pct': {'$multiply': [{'$divide': ['$value', num_cafes]}, 100]}}}, \n",
    "            {'$sort': {'pct': -1, '_id': 1}}, \n",
    "            {'$project': {'pct': {'$concat': [{'$substr': ['$pct', 0, 5]}, '%']}}}]\n",
    "\n",
    "pprint.pprint(list(ctp.aggregate(pipeline)))\n",
    "```\n",
    "```\n",
    "Out: [{'_id': 'created', 'pct': '100%'},\n",
    "      {'_id': 'id', 'pct': '100%'},\n",
    "      {'_id': 'tag', 'pct': '100%'},\n",
    "      {'_id': 'name', 'pct': '98.42%'},\n",
    "      {'_id': 'pos', 'pct': '96.84%'},\n",
    "      {'_id': 'cuisine', 'pct': '48.42%'},\n",
    "      {'_id': 'address', 'pct': '35.26%'},\n",
    "      {'_id': 'opening_hours', 'pct': '14.21%'},\n",
    "      {'_id': 'website', 'pct': '12.63%'},\n",
    "      {'_id': 'phone', 'pct': '10%'},\n",
    "      {'_id': 'internet_access', 'pct': '8.947%'},\n",
    "      {'_id': 'smoking', 'pct': '5.263%'},\n",
    "      {'_id': 'takeaway', 'pct': '5.263%'},\n",
    "      {'_id': 'source', 'pct': '4.736%'},\n",
    "      {'_id': 'wheelchair', 'pct': '4.736%'},\n",
    "      {'_id': 'wifi', 'pct': '4.736%'},\n",
    "      {'_id': 'building', 'pct': '3.157%'},\n",
    "      {'_id': 'node_refs', 'pct': '3.157%'},\n",
    "      {'_id': 'outdoor_seating', 'pct': '2.631%'},\n",
    "      {'_id': 'toilets', 'pct': '1.578%'},\n",
    "      {'_id': 'wikidata', 'pct': '1.578%'},\n",
    "      {'_id': 'designation', 'pct': '1.052%'},\n",
    "      {'_id': 'diaper', 'pct': '1.052%'},\n",
    "      {'_id': 'drive_through', 'pct': '1.052%'},\n",
    "      {'_id': 'email', 'pct': '1.052%'},\n",
    "      {'_id': 'operator', 'pct': '1.052%'},\n",
    "      {'_id': 'shop', 'pct': '1.052%'},\n",
    "      {'_id': 'brand', 'pct': '0.526%'},\n",
    "      {'_id': 'coffee', 'pct': '0.526%'},\n",
    "      {'_id': 'created_by', 'pct': '0.526%'},\n",
    "      {'_id': 'drinking_water', 'pct': '0.526%'},\n",
    "      {'_id': 'level', 'pct': '0.526%'},\n",
    "      {'_id': 'note', 'pct': '0.526%'},\n",
    "      {'_id': 'wikipedia', 'pct': '0.526%'}]\n",
    "``` \n",
    "\n",
    "reveals that most tags apply only to a fraction of the existing locations in a particular category (cafes in this example).  This reduces the usefulness of doing search queries against such data, as sampling errors (whether due to small samples for a given tag or large samples with selection bias) hamper the ability to extract useful statistics.\n",
    "  \n",
    "Applying machine vision recognition techniques to satellite imagery, one could identify likely buildings, landmarks, and points-of-interest.  Then, a smartphone app could detect when a user is near untagged or under-tagged locations and supply a notification to the user asking them to answer a question.  This question would either ask users to confirm a location's tag information or ask the user to supply new tag information, such as the type of amenity or the name of a restaurant.  Location-time distributions could also be used to detect areas of interest, as people tend to spend more time at a location than they do just traveling between locations (i.e. areas of low motion, or change in position over time, may reflect a location of interest).  This could be combined with machine vision techniques to make sure that question answering is targeted towards the most popular areas (since users' time and/or willingness to answer questions is an exhaustive resource).  In the case that machine vision techniques are too expensive or there is a lack of organizational expertise, out-of-the-box clustering algorithms applied to location-time data (gathered by smartphone GPS) would provide an approachable way for non-domain experts to extract points-of-interest.\n",
    "  \n",
    "### 3B. Pros and Cons\n",
    "\n",
    "Proactive, crowdsourced information gathering would allow OSM to leverage the power of the crowd as well as become more responsive over time, in the case that tag information changes (e.g. when a building's occupant changes).  Since some areas are under rapid redevelopment, tag information can quickly become stale, reducing its usefulness to consumers.  Currently, very few users are generating the majority of the OSM data (i.e. the Pareto Principle), likely due to the lack of an intuitive interface and locality in space (i.e. gathering data when it is most available and accurate).  A Q&A based, location-aware service would ameliorate these two issues.\n",
    "\n",
    "The main drawback of this approach is getting users to actually submit accurate data upon request.  Many users may be too busy or not care enough about the OSM mission to supply answers (or even worse, inaccurate answers).  One way to address the latter issue may be to match a user's answers against the answers of other users, to see how reliable a person's answers are (i.e. how well they correspond to the 'wisdom of the crowd').  The former problem could be alleviated by leveraging existing social media platforms to publicize a person's contributions.  While one may not expect virality from such an approach, using social media may allow such an app to reach the critical mass needed to sustain the information gathering process.  If enough users are contributing, the app could even create its own internal social network or community to motivate users to contribute more and work together on larger projects (e.g. Wikipedia has a community of contributors known as the Wikimedia movement)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
